%%%-*-latex-*-

\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage[charter]{mathdesign}
\usepackage{hyphenat}
\usepackage{mdwlist}              % Tight vertical spacing of list items
\usepackage{url}
\usepackage{microtype}
\usepackage{framed}

\frenchspacing  % Follow French conventions after a period
\hfuzz 2pt      % Do not report overhang less than 2pt

% Figures/Graphics
%
\usepackage{graphicx}   % Inclusion
\usepackage{float}      % Before package `wrapfig'
\usepackage[justification=raggedleft]{caption}
\usepackage{subfig}     % Subfigures
\captionsetup[subfloat]{justification=raggedleft}
\usepackage{wrapfig}    % Wrapping text around figures

% Maths & Logic
%
\usepackage{amsmath,amssymb,amsthm,stmaryrd}

% Bibliographic style
%
\usepackage[comma]{natbib}
\bibliographystyle{plainnat}
\usepackage{etoolbox}
\apptocmd{\thebibliography}{\raggedright}{}{} % No Underfull \hbox

% Verbatim and tty
%
\usepackage{verbatim} % Inputting in verbatim environment
\usepackage{alltt}    % Extended verbatim environment

% Tables
%
\usepackage{array}
\usepackage{multirow}
\usepackage{tabulary}
\usepackage{hhline}

% Miscellanea
%
\usepackage{xspace}    % Automatic insertion of a space after a macro
\usepackage{varioref}  % Qualifying references with page numbers
\usepackage{booktabs}
\usepackage{clrscode}

% Commands
%
\input{commands}
\input{ocaml_syntax}
\input{ocaml_macros}
\input{comp_macros}
\input{token}
\input{grammar}
\input{dfa_macros}

% Title
%
\title{The front-end of the LIGO compiler}
\author{\Large Christian Rinderknecht}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  This article belongs to a series aimed at presenting the
  architecture, concepts, techniques and features of the
  front\hyp{}end of the LIGO compiler. It also aims at explaining some
  of the theoretical background and the tools used in making up the
  front\hyp{}end.
\end{abstract}

\section{An overview of compilation}

Generally speaking, a \emph{compiler} maps programs written in a
\emph{source language} into programs written in a \emph{target
  language}. If the source and the target are ascribed their own
notion of the meaning of their programs, then compilation is expected
to preserve the meaning of the source. Meaning is understood here as
operational behaviour, therefore compilation entails that the outputs
of the execution of the source program are the same as the outputs of
the execution of the target program, given the same inputs in the same
order.\footnote{There are other ways to define the semantics of a
  programming language, like denotational semantics or axiomatic
  semantics.} If the target language has some operational semantics
but the source language has none, then compilation actually gives
meaning to the source by translation.

A peculiarity of LIGO is that it features multiple source languages
which are all translated to the same internal representation (IR). One
could actually argue that LIGO as a language \emph{is} that IR, and
that the different source languages are pure syntax being given
meaning by translation to the IR, whose semantics is, in turn, given
by translation up to Michelson. The target language of the LIGO
compiler is Michelson, used to write \emph{smart contracts} for the
Tezos blockchain. Michelson itself is \emph{interpreted}, that is, the
meaning of a smart contract is its execution by another program,
called an \emph{interpreter}, instead of a direct execution by the
computer's hardware. Some languages can be both compiled or
interpreted, or both. One of such languages is OCaml, whose programs
are compiled to a low-level language like Michelson, called
\emph{bytecode}, which is then either compiled to native code or
interpreted. Another example of such a language is Java.

A \emph{transpiler} is a compiler whose target language is of the same
\emph{level of abstraction} as the source language, that is, it
features almost the same constructs or concepts, like loops,
functions, data types, modules etc., as the source language, even if
the concrete syntax is different. For example, a compiler from a a
dialect of Lisp to another is likely to be a transpiler because both
languages likely feature almost the same kind of constructs. But
Pascal and Ada are essentially different because Ada features tasks,
which have no equivalent in Pascal by translation.\footnote{This
  notion of level of abstraction cannot be taken too seriously,
  though, as, in the end, we are dealing here with
  Turing\hyp{}complete languages, so we can theoretically always
  interpret in a language the programs of another.}

There are currently four concrete syntax to the LIGO compiler, each
based on an existing programming language: PascaLIGO, CameLIGO,
ReasonLIGO and JsLIGO (respectively based on Pascal, OCaml, ReasonML
and JavaScript). Another peculiarity of LIGO is that one can transpile
a program in any of those syntaxes into any of the other. Technically
speaking, a bit of compilation is involved sometimes, that is, more
than a syntactical transformation, for example when the source
language features loops, like JsLIGO, and the target language does
not, like CameLIGO. We will call this process transpilation
nonetheless.

From a theoretical point of view, a compiler for a general language is
only one link in a chain of tools, as shown in
\fig~\vref{fig:compilation_chain}.
\begin{figure}
\centering
\includegraphics[bb=71 587 405 721]{compilation_chain}
\caption{Compilation chain for~C\label{fig:compilation_chain}}
\end{figure}
Let us consider the example of the C~language. A widely used
open\hyp{}source compiler is GNU GCC. In reality, GCC is a host of
tools making up a complete compilation chain, not just a naked
C~compiler\footnote{and not just for~C either: Ada too, for example.}:
\begin{itemize*}

  \item to only preprocess the sources: \texttt{gcc -E prog.c}
  (standard output) (the C~preprocessor \texttt{cpp} can also be
  called directly);

  \item to preprocess and compile: \texttt{gcc -S prog.c}
  (output \texttt{prog.s});

  \item to preprocess, compile and assemble: \texttt{gcc -c prog.c}
  (out: \texttt{prog.o});

  \item to preprocess, compile, assemble and link: \texttt{gcc -o prog
    prog.c} (output \texttt{prog}). Linking can be directly called
    using~\texttt{ld}.

\end{itemize*}
Like GCC, the LIGO compiler actually contains several tools, not just
a compiler. It is simpler though, because there is neither need for an
assembler nor a linker, as Michelson is interpreted and not further
lowered down. As we shall see, LIGO features a preprocessor modelled
on that of~C, also integrated into the ``compiler'' and can be invoked
independently of the rest of the compiler, just like transpilation
between concrete syntaxes.

There are two parts to compilation: \emph{analysis} and
\emph{synthesis}.
\begin{enumerate*}

  \item The analysis part breaks up the source program into
  constituent pieces of an intermediary representation of the
  program.

  \item The synthesis part constructs the target program from this
  intermediary representation.

\end{enumerate*}
In practice, there are more than one IR, and the LIGO compiler
internally defines more than five IR at the time of writing.


Here, we shall concern ourselves only with analysis, which can itself
be divided into three successive stages:
\begin{enumerate*}

  \item \emph{linear analysis}, also commonly called lexical analysis
    or lexing, in which the stream of characters making up the source
    program is read and grouped into \emph{lexemes}, that is,
    sequences of characters having a collective meaning: sets of
    lexemes with a common interpretation are called \emph{tokens}.
    Note that ``token'' is often used when ``lexeme'' would be
    correct, but the confusion is minimal and we will rely on it to
    make the presentation better flow: lexers will output streams of
    tokens.

  \item \emph{hierarchical analysis}, also commonly called syntactical
    analysis or parsing, in which tokens are grouped hierarchically
    into nested collections (trees) with a collective meaning;

  \item \emph{semantic analysis}, also commonly called type inference
    of type checking, in which certain checks are performed on the
    previous hierarchy to ensure that the components of a program fit
    together meaningfully.

\end{enumerate*}

In LIGO, the \emph{front\hyp{}end} is the part of the compiler dealing
with preprocessing, lexing and parsing, as well as transpilation
between the concrete syntaxes. Type inference and checking is
considered part of the middle\hyp{}end of the LIGO compiler, before
optimisation and code generation.

In the following, we shall focus on lexing and scanning analysis,
following examples from~\cite{Dragon_1986} but the presentation is
ours.

\paragraph{Lexical analysis}

In a compiler, linear analysis is called \emph{lexing} or
\emph{scanning}. During lexical analysis, the characters in the
following PascaLIGO assignment
\begin{verbatim}
position := initial + rate * 60
\end{verbatim}
\noindent would be grouped into the following lexemes and tokens (see
table). The blanks separating the characters of these tokens are
normally discarded.
\begin{center}
\begin{tabular}{l|>{\tt}l}
\toprule
  \multicolumn{1}{c}{\textsc{Token}}
& \multicolumn{1}{c}{\textsc{Lexeme}}\\
\midrule
identifier & position\\
assignment symbol & :=\\
identifier & initial\\
plus sign & +\\
identifier & rate\\
multiplication sign & *\\
number & 60\\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Syntax analysis}

Hierarchical analysis is called \emph{parsing} or \emph{syntax
  analysis}. It involves grouping the tokens of the source program
into grammatical phrases that are used by the compiler to synthesise
the output. Usually, the grammatical phrases of the source are
represented by a \emph{parse tree} such as in
\fig~\vref{fig:parse_tree_eg}.
\begin{figure}[b]
\centering
\includegraphics[bb=71 603 338 721]{parse_tree_eg}
\caption{Parse tree of \texttt{position := initial + rate * 60}\label{fig:parse_tree_eg}}
\end{figure}
In the expression ``\texttt{initial + rate * 60}'', the phrase
``\texttt{rate * 60}'' is a logical unit because the usual conventions
of arithmetic expressions tell us that multiplication is performed
prior to addition. Thus, because the expression ``\texttt{initial +
  rate}'' is followed by a ``\verb+*+'', it is \emph{not} grouped into
the same subtree.

The hierarchical structure of a program is usually expressed by
\emph{recursive rules}. For instance, an expression can be defined by
a set of cases as follows:
\begin{enumerate*}

  \item any \emph{identifier} is an expression;\label{rule_id_is_expr}

  \item any \emph{number} is an expression;\label{rule_num_is_expr}

  \item if \emph{expression}\(_1\) and \emph{expression}\(_2\) are
  expressions, then so are
   \begin{enumerate*}

     \item \emph{expression}\(_1\) \verb|+|
       \emph{expression}\(_2\), \label{rule_add_is_expr}

     \item \emph{expression}\(_1\) \verb|*|
       \emph{expression}\(_2\), \label{rule_mult_is_expr}

     \item (\emph{expression\(_1\)}).

   \end{enumerate*}

\end{enumerate*}
Rules~\ref{rule_id_is_expr} and~\ref{rule_num_is_expr} are
non-recursive base rules, while the others define expressions in terms
of operators applied to other expressions:
\begin{itemize*}

  \item ``\texttt{initial}'' and ``\texttt{rate}'' are identifiers,
    therefore, by rule~\ref{rule_id_is_expr}, they are expressions;

  \item ``\texttt{60}'' is a number, thus, by
    rule~\ref{rule_num_is_expr}, it is an expression.

\end{itemize*}
Next, by rule~\ref{rule_mult_is_expr}, we infer that ``\texttt{rate *
  60}'' is an expression. Finally, by rule~\ref{rule_add_is_expr}, we
conclude that ``\texttt{initial + rate * 60}'' is an
expression. Similarly, many programming languages define statements
recursively by rules such as:
\begin{itemize}

  \item if \emph{identifier} is an identifier and
    \emph{expression} is an expression, then we can form the statement
    \begin{center}
      \emph{identifier} \texttt{:=} \emph{expression}
    \end{center}

  \item if \emph{expression} is an expression and \emph{statement} is
    a statement, then we can create the statements
    \begin{center}
     \tt while (\emph{expression}) do \emph{statement}\\
     if (\emph{expression}) then \emph{statement}
    \end{center}

\end{itemize}
Let us keep in mind that the distinction between lexical and syntactic
analysis is somewhat arbitrary. For instance, we could define the
integer numbers by means of the following recursive rules:
\begin{itemize*}

  \item a \emph{digit} is a \emph{number} (base rule),

  \item a \emph{digit} followed by a \emph{number} is a \emph{number}
    (recursive rule).

\end{itemize*}
Imagine now that the lexer does \emph{not} recognise numbers, only
digits. The parser therefore would use the previous recursive rules to
group in a parse tree the digits which form a number. For instance,
the parse tree for the number \texttt{1234}, following these rules, is
shown in \fig~\vref{fig:num_parse_tree}.
\begin{figure}
\centering
\subfloat[Parse tree\label{fig:num_parse_tree}]{
  \includegraphics[bb=71 602 218 721]{num_parse_tree}}
\quad
\subfloat[Abstract syntax tree\label{fig:ast_eg}]{
  \includegraphics[bb=71 602 187 721]{ast_eg}
}
\caption{Parse tree vs. abstract syntax tree}
\end{figure}
Notice how that tree actually is isomorphic to a list and the
structure, \emph{i.e.,} the embedding of trees, reflects the
distinction of thousands, hundreds, tens and units. This level of
detail is usually not necessary, as it can be obtained by purely
arithmetic means from a number (as opposed to a tree). Nevertheless,
there is an obvious benefit to using the parser for the lexical
analysis. As we shall see in the case of the LIGO compiler (but not
the Michelson interpreter), the source code of lexers is often
generated from a higher\hyp{}level description of what it does, and
partly how. So there is a practical and theoretical benefit to
\emph{scannerless parsing} in using only one framework. Currently,
though, the LIGO compiler currently only uses a lexer generator called
\ocamllex.

Pragmatically, the best division between the lexer and the parser is
the one that simplifies the overall task of analysis. One factor in
determining the distinction is whether a source language construct
requires general recursion, as opposed to \emph{tail recursion}
(see~\cite{Rinderknecht_2014a}) or if it features some internal
structure: lexical constructs do not require general recursion to be
defined, and tokens are usually considered atomic. This distinction
may seem like hair splitting, but it became relevant in the LIGO
compiler with respect to the embedding of Michelson code.

On the other hand, the kind of linear scanning described above is not
powerful enough to analyse expressions or statements, like matching
parentheses in expressions, or matching braces in block statements: a
nesting structure is compulsory, as seen earlier in
\fig~\vref{fig:parse_tree_eg}. The internal representation of this
syntactic structure is called an \emph{abstract syntax tree} (or
simply \emph{syntax tree}), an example of which can be seen in
\fig~\vref{fig:ast_eg}. It is a compressed version of the parse tree,
where only the most important elements are retained for the semantic
analysis.

\paragraph{Phases}

Conceptually, a compiler operates in \emph{phases}, also called
\emph{passes}, each transforming the program from one representation
to the next, each step closer to the target program and thereby
generating a \emph{pipeline architecture}. This decomposition is shown
in \fig~\vref{fig:phases}.
\begin{figure}
\centering
\includegraphics[bb=71 618 408 721]{phases}
\caption{Decomposition of a compiler into phases\label{fig:phases}}
\end{figure}
The first row makes up the analysis and the second is the synthesis.

\paragraph{Lexing}

Let us revisit the analysis phase and its sub-phases, following up on
a previous example. Consider the following character string:
\begin{center}
\(\longleftarrow\)
\texttt{
  \begin{tabular}{|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|}
    \hline
    p&o&s&i&t&i&o&n&\phantom{=}&:&=&\phantom{=}&i&n&i&t&i&a&l&
    \phantom{=}&+&\phantom{=}&r&a&t&e&\phantom{=}&*&\phantom{=}&6&0\\
    \hline
  \end{tabular}
}
\(\longleftarrow\)
\end{center}
First, as we stated earlier, lexical analysis recognises the tokens of
this character string, which can be stored in a file. Lexing results
in a stream of tokens like
\begin{center}
\raggedleft
\boxedToken{ID}{position} \boxedToken{SYM}{:=}
\boxedToken{ID}{initial} \boxedToken{OP}{+} \boxedToken{ID}{rate}
\boxedToken{OP}{*} \boxedToken{NUM}{60}
\end{center}
where \tokenName{ID} (\emph{identifier}), \tokenName{SYM}
(\emph{symbol}), \tokenName{OP} (\emph{operator}) and \tokenName{NUM}
(\emph{number}) are the token names and between brackets are the
\emph{lexemes}. Each token also includes the \emph{region} of the
corresponding lexeme in the input string. Such a region is made of a
couple of character location. A location is either determined by the
number of characters since the beginning of the input string, or the
line number and offset on the line (or column number).

\paragraph{Parsing}

The parser takes this token stream and outputs the corresponding
syntax tree and/or report errors. In \fig~\vref{fig:ast_eg}, we gave a
simplified version of this syntax tree. A refined version is given in
the facing column.
\begin{center}
  \includegraphics[bb=71 619 218 721]{refined_ast_eg}
\end{center}
The parse tree can be considered as a \emph{trace} of the parsing
pass: it summarises all the recognition work done by the parser. It
depends on the syntax rules (\emph{i.e.,} the grammar) and the input
stream of tokens.
\begin{center}
\includegraphics[bb=71 605 377 721]{refined_parse_tree_eg}
\end{center}

\section{Theory of lexical analysis}

In this section, we explain how lexing works based on an input buffer
of characters and a transition diagram (a special case of finite
automaton). The usual presentation uses two buffers, but ours only
one, at the expense of one more pointer to buffer characters. Through
examples and visual representations, we hope it will contribute to
building a good mental model when writing lexers.

\paragraph{Input buffer}

The stream of characters that provides the input to the lexer comes
usually from a file. For efficiency reasons, when this file is opened,
a \emph{buffer} is associated, so the lexer actually reads its
characters from this buffer in memory. A buffer is like a
\emph{queue}, or \emph{FIFO} (\emph{First in, First out}), that is, a
list whose one end is used to put elements in and whose other end is
used to get elements out, one at a time. The only difference is that a
buffer has a \emph{fixed size} (hence a buffer can be full). An empty
buffer of size three is depicted as follows:
\begin{center}
output side
\(\longleftarrow\)
\begin{tabular}{|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|}
  \hline
  \phantom{=}
& \phantom{=}
& \phantom{=}\\
  \hline
\end{tabular}
\(\longleftarrow\)
input side
\end{center}
If we input characters \exc{A} then \exc{B} in this buffer, we draw
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{A}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{B}}
& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
The symbol~\(\upharpoonright\) is a pointer to the next character
available for output. Let us keep in mind that the blank character
will now be noted ``\texttt{\char`\ }'', in order to avoid confusion
with an empty cell in a buffer. So, if we input now a blank in our
buffer from the file, we get the full buffer
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{A}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{B}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
and no more inputs are possible until at least one output is done. Let
us be careful: a buffer is full if and only if~\(\upharpoonright\)
points to the leftmost character. For example,
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{A}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{B}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
is \emph{not} a full buffer: there is still room for one character. If
we input~\exc{C}, it becomes
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{B}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{C}}

& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
which is now a full buffer. The overflowing character \exc{A} has been
discarded. Now, if we output a character (or, equivalently, the lexer
inputs a character) we get
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{B}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{C}}

& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
Let us output another character:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{B}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{C}}

& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
Now, if the lexer needs a character, \exc{C} is output and some
routine automatically reads some more characters from the disk and
fill them in order into the buffer. This happens when we output the
rightmost character. Assuming the next character in the file is
\exc{D}, after outputting \exc{C} we get
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{C}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{D}}
& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
If the buffer only contains the \emph{end-of-file} character (noted
here \eof), it means that no more characters are available from the
file. So, if we have the situation
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}ccl}
  \cline{3-4}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{@{\,}c@{\,}|}{\(\cdots\)}
& \multicolumn{1}{@{\,}c@{\,}|}{\eof}
& \(\longleftarrow\)
& empty file\\
  \cline{3-4}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
in which the lexer requests a character, it would get \eof and
subsequent requests would fail, because both the buffer and the file
would be empty.

\mypar{Transition diagrams}

As an intermediary step in the construction of a lexical
analyser,\label{transition} we introduce another concept, called
\emph{transition diagram}. Transition diagrams depict the actions that
take place when a lexer is called by a parser to get the next token.
\emph{States} in a transition diagram are drawn as circles. Some
states have double circles, with or without an
asterisk~\textsc{*}. States are connected by arrows, called
\emph{edges}, each one carrying an input character as \emph{label}, or
the special label~\other. An example of such transition diagram is
given in \fig~\vref{fig:dfa_geq}.
\begin{figure}
\centering
\includegraphics[bb=60 660 190 730]{dfa_geq}
\caption{A transition diagram\label{fig:dfa_geq}}
\end{figure}
Double-circled states are called \emph{final states}. The special
arrow which does not connect two states points to the \emph{initial
  state}. A state in the transition diagram corresponds to the state
of the input buffer, \emph{i.e.,} its contents and the output pointer
at a given moment. At the initial state, the buffer contains at least
one character. If the only one remaining character is \eof, the lexer
returns a special token \term{\$} to the parser and stops. Let us
assume that the character~\(c\) is pointed by~\(\upharpoonright\) in
the input buffer and that \(c\)~is not \eof, depicted as follows:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-5}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{@{\,}c@{\,}|}{\(\cdots\)}
& \multicolumn{1}{@{\,}c@{\,}|}{\(c\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-5}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
When the parser requests a token, if an edge to the state~\(s\) has a
label with character~\(c\), then the current state in the transition
diagram becomes~\(s\), and \(c\)~is removed from the buffer. This is
repeated until a final state is reached or we get stuck. If a final
state is reached, it means the lexer recognised a token --~which is in
turn returned to the parser. Otherwise a lexical error occurred.

Let us consider again the diagram in \fig~\vref{fig:dfa_geq} and let
us assume that the initial input buffer is
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
From the initial state~\(1\) to the state~\(2\) there is an arrow with
the label ``\exc{>}''. Because this label is present at the output
position of the buffer, we can change the diagram state to~\(2\) and
remove ``\exc{<}'' from the buffer, which becomes
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
From state~\(2\) to state~\(3\) there is an arrow with label
``\exc{=}'', so we remove it:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\texttt{\char`\ }}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
and we move to state~\(3\). Since state~\(3\) is a final state, we are
done: we recognised the token \token{relop}{>=}. Let us imagine now
the input buffer is
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{2}}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
In this case, we will move from the initial state to state~\(2\):
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{2}}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
We cannot use the edge with label ``\exc{=}'', but we can use the one
with ``\other''. Indeed, the ``\other'' label refers to any character
that is not indicated by any of the edges leaving the state. Hence, we
move to state~\(4\) and the input buffer becomes
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{2}}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
and the lexer emits the token \token{relop}{>}. But there is a problem
here: if the parser requests another token, we have to start again
with this buffer but we already skipped the character
\begin{tabular}{|@{\,}c@{\,}|}
\hline
\exc{1}\\
\hline
\end{tabular}
and we forgot where the recognised lexeme starts. The idea is to use
another arrow to mark the starting position when we try to recognise a
token. Let~\(\upharpoonleft\) be this new pointer. Then the initial
buffer of our previous example would be depicted as
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
& \multicolumn{1}{@{}c@{}}{\(\upharpoonleft \upharpoonright\)}
\end{tabular}
\end{center}
When the lexer reads the next available character, the
pointer~\(\upharpoonright\) is shifted to the right of one position.
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
We are now at state~\(2\) and the current character, that is, pointed
by~\(\upharpoonright\), is~\exc{1}. The only way to continue is to go
to state~\(4\), using the special label \other. The pointer of the
secondary buffer shifts to the right and, since it points to the last
position, we input one character from the primary buffer:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
State~\(4\) is a final state a bit special: it is marked with
`\emph{\textsc{*}}'. This means that before emitting the recognised
lexeme we have to shift the current pointer by one position \emph{to
  the left}:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-7}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\phantom{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{>}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-7}
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
\emph{This allows to recover the character}
\begin{tabular}{|@{\,}c@{\,}|}
\hline
\exc{1}\\
\hline
\end{tabular}
\emph{as current character.} Moreover, the recognised lexeme now
always starts at the pointer~\(\upharpoonleft\) and ends one position
before the pointer~\(\upharpoonright\). So, here, the lexer outputs
the lexeme `\exc{>}'. Actually, we can complete our token
specification by adding some extra information that are useful for the
recognition process, as just described. First, it is convenient for
some tokens, like \tokenName{relop}, not to carry the lexeme verbatim,
but a symbolic name instead, which is independent of the actual size
of the lexeme. For instance, we shall write \itoken{relop}{gt} instead
of \token{relop}{>}. Second, it is useful to write the recognised
token and the lexeme close to the final state in the transition
diagram itself. See \fig~\vref{fig:dfa_geq_completed}.
\begin{figure}
\centering
\includegraphics[bb=55 660 224 730]{dfa_geq_completed}
\caption{Completion of \fig~\vref{fig:dfa_geq}
\label{fig:dfa_geq_completed}}
\end{figure}
Similarly, \fig~\vref{fig:dfa_relop}
\begin{figure}
\centering
\includegraphics[bb=50 600 265 745]{dfa_relop}
\caption{Relational operators\label{fig:dfa_relop}}
\end{figure}
shows all the relational operators.

\mypar{Identifiers and longest prefix match}

A transition diagram for specifying identifiers is given in \fig~\vref{fig:dfa_ident}.
\begin{figure}
\centering
\includegraphics[bb=48 684 260 755]{dfa_ident}
\caption{Transition diagram for identifiers\label{fig:dfa_ident}}
\end{figure}
\texttt{lexeme} is a function call which returns the recognised
lexeme, as found in the \texttt{buffer}. The \other label on the last
step to final state force the identifier to be of \emph{maximal
  length}. For instance, given \texttt{counter+1}, the lexer will
recognise \texttt{counter} as identifier and not just
\texttt{count}. This is called \emph{the longest prefix} property.

\mypar{Keywords}

Since keywords are sequences of letters, they are exceptions to the
rule that a sequence of letters and digits starting with a letter is
an identifier. One solution for specifying keywords is to use
dedicated transition diagrams, one for each keyword. For example, the
\term{if} keyword is simply specified in \fig~\vref{fig:dfa_if}.
\begin{figure}[b]
\centering
\includegraphics[bb=47 711 192 730]{dfa_if}
\caption{Transition diagram for \term{if}\label{fig:dfa_if}}
\end{figure}
If one keyword diagram succeeds, \emph{i.e.,} the lexer reaches a
final state, then the corresponding keyword is transmitted to the
parser; otherwise, another keyword diagram is tried after shifting the
current pointer~\(\upharpoonright\) in the input buffer back to the
starting position, \emph{i.e.,} pointed by~\(\upharpoonleft\).

There is a problem, though. Consider the \OCaml language, where there
are two keywords \term{fun} and \term{function}. If the diagram of
\term{fun} is tried successfully on the input \texttt{function} and
then the diagram for identifiers, the lexer outputs the lexemes
\term{fun} and \token{id}{ction} instead of one keyword
\term{function}. As for identifiers, we want the longest prefix
property to hold for keywords too and this is simply achieved by
\emph{ordering the transition diagrams}. For example, the diagram of
\term{function} must be tried before the one for \term{fun} because
\term{fun} is a prefix of \term{function}. This strategy implies that
the diagram for the identifiers (given in \fig~\ref{fig:dfa_ident} on
page~\pageref{fig:dfa_ident}) must appear \emph{after} the diagrams
for the keywords.

There are still several drawbacks with this technique, though. The
first problem is that if we indeed have the longest prefix property
among keywords, it does not hold with respect to the identifiers. For
instance, \texttt{iff} would lead to the keyword \term{if} and the
identifier \texttt{f}, instead of the (longest and sole) identifier
\texttt{iff}. This can be remedied by forcing the keyword diagram to
recognise a keyword and not an identifier. This is done by failing if
the keyword is followed by a letter or a digit (remember we try the
longest keywords first, otherwise we would miss some keywords --- the
ones which have prefix keywords). The way to specify this is to use a
special label \compl such that \compl \(c\) denotes the set of
characters which are \emph{not} \(c\). Actually, the special label
\other can always be represented using this \compl label because
\other means `not the others labels.' Therefore, the completed
\term{if} transition diagram would be as found in
\fig~\vref{fig:dfa_if_completed}.
\begin{figure}
\centering
\includegraphics[bb=48 711 261 738]{dfa_if_completed}
\caption{Completion of \fig~\vref{fig:dfa_if}
\label{fig:dfa_if_completed}}
\end{figure}
where \term{alpha} is either \term{letter} or \term{digit}.

The second problem with this approach is that we have to create a
transition diagram for each keyword and a state for each of their
letters. In real programming languages, this means that we get
hundreds of states only for the keywords. This problem can be avoided
if we change our technique and give up the specification of keywords
with transition diagrams.
\begin{center}
\begin{tabular}{>{\tt}ll}
\toprule
  \multicolumn{2}{c}{Keywords}\\
\midrule
  \multicolumn{1}{c}{Lexeme}
& \multicolumn{1}{c}{Token}\\
\hline \hline
if   & \tokenName{if}\\
then & \tokenName{then}\\
else & \tokenName{else}\\
\bottomrule
\end{tabular}
\end{center}
Since keywords are a strict subset of identifiers, let us use only the
identifier diagram but \emph{we change the action at the final state},
\emph{i.e.,} instead of always returning an~\tokenName{id} token, we
lookup the recognised string in the table of keywords: if not found,
it is an identifier. Let us call \texttt{switch} the function which
makes this decision based on the buffer (equivalently, the current
diagram state) and a table of keywords. The specification is shown in
\fig~\vref{fig:dfa_id_kwd}.
\begin{figure}[b]
\centering
\includegraphics[bb=48 684 285 755]{dfa_id_kwd}
\caption{Transition diagram for keywords\label{fig:dfa_id_kwd}}
\end{figure}

In the same vein, let us consider now the numbers as specified by the
transition diagram in \fig~\vref{fig:dfa_num}
\begin{figure}
\centering
\includegraphics[bb=47 627 320 746]{dfa_num}
\caption{Transition diagram for numbers\label{fig:dfa_num}}
\end{figure}
and white space specified by the transition diagram in
\fig~\vref{fig:dfa_ws}.
\begin{figure}
\centering
\includegraphics[bb=48 711 190 755]{dfa_ws}
\caption{Transition diagram for white space\label{fig:dfa_ws}}
\end{figure}
The specificity of the latter diagram is that there is no action
associated to the final state: no token is emitted.

\paragraph{A simplification}

There is a simple away to reduce the size of the diagrams used to
specify the tokens while retaining the longest prefix property: allow
to pass through several final states. This way, we can actually also
get rid of the `\emph{\textsc{*}}' marker on final states. Coming back
to the first example in \fig~\vref{fig:dfa_geq_completed}, we would
alternatively make up \fig~\vref{fig:dfa_geq_opt}.
\begin{figure}
\centering
\includegraphics[bb=48 702 220 730]{dfa_geq_opt}
\caption{Alternative to \fig~\vref{fig:dfa_geq_completed}
\label{fig:dfa_geq_opt}}
\end{figure}
But we have to change the recognition process a little bit here in
order to keep the longest prefix match: we do not want to stop at
state~\(2\) if we could recognise `\texttt{>=}'.

The simplified complete version with respect to the one given in
\fig~\vref{fig:dfa_relop} is found in \fig~\vref{fig:dfa_relop_opt}.
\begin{figure}
\centering
\includegraphics[bb=25 600 200 745]{dfa_relop_opt}
\caption{Simplification of \fig~\vref{fig:dfa_relop}
\label{fig:dfa_relop_opt}}
\end{figure}
The transition diagram for specifying identifiers \emph{and} keywords
looks now like \fig~\vref{fig:dfa_id_kwd_opt}.
\begin{figure}
\centering
\includegraphics[bb=48 682 208 756]{dfa_id_kwd_opt}
\caption{Simplification of \fig~\vref{fig:dfa_id_kwd}
\label{fig:dfa_id_kwd_opt}}
\end{figure}
The transition diagram for specifying numbers is simpler now, as seen
in \fig~\vref{fig:dfa_num_opt}.
\begin{figure}
\centering
\includegraphics[bb=48 632 320 758]{dfa_num_opt}
\caption{Simplification of \fig~\vref{fig:dfa_num}
\label{fig:dfa_num_opt}}
\end{figure}

How do we interpret these new transition diagrams, where the final
states may have out\hyp{}going edges (and the initial state have
incoming edges)? For example, let us consider the recognition of a
number:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\upharpoonright\)}
\end{tabular}
\end{center}
As usual, if there is a label of an edge going out of the current
state which matches the current character in the buffer, the
pointer~\(\upharpoonright\) is shifted to the right of one
position. The new feature here is about final states. When the current
state is final
\begin{enumerate*}

  \item the current position in the buffer is pointed to with a new
  pointer~\(\Uparrow\);

  \item if there is an out-going edge which carries a matching
  character, we try to recognise a longer lexeme;
  \begin{enumerate*}

    \item if we fail, \emph{i.e.,} if we cannot go further in the
      diagram and the current state is not final, then we shift back
      the current pointer~\(\upharpoonright\) to the position pointed
      by~\(\Uparrow\);

    \item and return the then-recognised token and lexeme;

  \end{enumerate*}

  \item if not, we return the recognised token and lexeme associated
  to the current final state.

\end{enumerate*}
Following our example of number recognition:
\begin{itemize}

  \item The label \term{digit} matches the current character in the
  buffer, \emph{i.e.,} the one pointed by~\(\upharpoonright\), so we move to
  state~\(2\) and we shift right by one the
  pointer~\(\upharpoonright\).
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
  \item The state~\(2\) is final, so we set the pointer~\(\Uparrow\)
  to the current position in the buffer
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
& \multicolumn{1}{@{}c@{}}{\(\Uparrow\upharpoonright\)}
\end{tabular}
\end{center}

  \item We shift right by one the current pointer and stay in
  state~\(2\) because the matching edge is a loop (notice that we did
  not stop here).
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\Uparrow\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}

  \item The state~\(2\) is final so we set pointer~\(\Uparrow\) to point to
    the current position:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
& \multicolumn{1}{@{}c@{}}{\(\Uparrow\upharpoonright\)}
\end{tabular}
\end{center}

  \item The \term{digit} label of the loop matches again the current
  character (here \exc{3}), so we shift right by one the current
  pointer.
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\Uparrow\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}

  \item Because state~\(2\) is final we set the pointer~\(\Uparrow\)
    to the current pointer~\(\upharpoonright\):
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{3}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
&
& \multicolumn{1}{@{}c@{}}{\(\Uparrow\upharpoonright\)}
\end{tabular}
\end{center}

  \item State~\(2\) is final, so it means that we succeeded in
    recognising the token associated with state~\(2\):
    \token{num}{lexeme(buffer)}, whose lexeme is between
    \(\upharpoonleft\)~included and~\(\upharpoonright\) excluded,
    \emph{i.e.,} \texttt{153}.

\end{itemize}
Let us consider the following initial buffer:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{.}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\upharpoonright\)}
\end{tabular}
\end{center}
Character~\texttt{1} is read and we arrive at state~\(2\) with the
following situation:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{.}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
& \multicolumn{1}{@{}c@{}}{\(\Uparrow\upharpoonright\)}
\end{tabular}
\end{center}
Then \texttt{5}~is read and we arrive again at state~\(2\) but we
encounter a different situation:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{.}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
& \multicolumn{1}{@{}c@{}}{\(\Uparrow\upharpoonright\)}
\end{tabular}
\end{center}
The label on the edge from state~\(2\) to~\(3\) matches `\exc{.}' so we
move to state~\(3\), shift by one the current pointer in the buffer:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{.}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\Uparrow\)}
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonright\)}
\end{tabular}
\end{center}
Now we are stuck at state~\(3\). Because this is not a final state, we
should fail, \emph{i.e.,} report a lexical error, but because the
pointer~\(\Uparrow\) has been set (\emph{i.e.,} we met a final state),
we shift the current pointer back to the position of the
pointer~\(\Uparrow\) and return the corresponding lexeme~\texttt{15}:
\begin{center}
\begin{tabular}{rcc@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}c@{\,}@{\,}ccl}
  \cline{3-10}
  lexer
& \(\longleftarrow\)
& \multicolumn{1}{|@{\,}c@{\,}|}{\exc{a}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{=}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{1}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{5}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{.}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{+}}
& \multicolumn{1}{@{\,}c@{\,}|}{\exc{6}}
& \multicolumn{1}{@{\,}c@{\,}}{\(\cdots\)}
& \(\longleftarrow\)
& file\\
  \cline{3-10}
&
&
&
& \multicolumn{1}{@{\,}c@{\,}}{\(\upharpoonleft\)}
&
& \multicolumn{1}{@{}c@{}}{\(\Uparrow\upharpoonright\)}
\end{tabular}
\end{center}

\section{Diagrams with \(\epsilon\)-transitions}

Transition diagrams as shown in \fig~\vref{fig:dfa_num} are relatively
complex to design. We often, at a given state, ``jump'' to another
without consuming any character in the character input buffer. We also
want to compose in parallel diagrams, meaning that, in a given state,
we either want to use this diagram, or this one, or that one,
etc. This can be achieved by introducting a special kind of character,
noted \(\epsilon\), as a label on the transitions. The interpretation
of this new kind of transition, called \(\epsilon\)-transition, is
that the current state changes by following this transition
\emph{without reading any input}. This is sometimes referred as a
\emph{spontaneous transition}.

For example, the \fig~\vref{fig:enfa_num} specifies signed natural and
decimal numbers by means of an \(\epsilon\)-diagram.
\begin{figure}
\centering
\includegraphics[bb=49 660 288 758]{enfa_num}
\caption{Signed natural and decimal numbers\label{fig:enfa_num}}
\end{figure}
This is not the simplest \(\epsilon\)-diagram we can imagine for these
numbers, but note the utility of the \(\epsilon\)-transition
from~\(q_4\) to~\(q_3\) (a jump).

In the case of lexers, \(\epsilon\)-transitions enable the separate
design of a diagram for each token, then the creating of an initial
(respectively, final) state connected to all their initial
(respectively, final) states with an \(\epsilon\)-transition.  As an
example of parallel composition, consider for instance the case of the
keywords \term{fun} and \term{function}, as well as identifiers:
\begin{center}
\includegraphics[bb=48 630 421 732,scale=0.88]{enfa_kwd_id}
\end{center}



The theory of automata tells us that, given a diagram with
\(\epsilon\)-transitions, there always exists an equivalent diagram
without \(\epsilon\)-transitions. There are some algorithms that
constitute a constructive proof of that theorem. The downside is that
the number of states in the resulting diagram is often greater than in
the \(\epsilon\)-diagram.\footnote{In theory, exponentially as many,
  but those cases are very rare in practice.} In some simple cases,
the number of states is smaller, as in


\section{From regular expressions to transition diagrams}

In section~\vref{transition}, we mentioned that transitions diagrams
are an intermediary step in the construction of lexers. Indeed, those
diagrams are graphical and graphical tools for building lexers are
lacking. Instead, we could textually define the diagrams by means of
transition tables, where each entry is a state and each column is a
character to be read in the input buffer. The issue would then be the
maintenance of such tables in a text format (think adding a row or a
column): this is really the realm of relational databases.

The prefered method is to define a transition diagram by means of a
\emph{regular expression}. A regular expression can be thought as a
program belonging to a language (a regular language), whose meaning is
(in this presentation) a transition diagram. There are multiple
syntaxes for thoses languages, so we will use minimal and abstract
syntax here. For example, the following regular expression
\begin{align*}
\text{\term{digit}\plus{} \lparen\exc{.}
  \term{digit}\plus\rparen\opt{} \lparen\exc{E} \lparen\exc{+} \disj
  \exc{-}\rparen\opt{} \term{digit}\plus\rparen\opt}
\end{align*}
specifies the transition diagram for numbers
in~\fig~\vref{fig:dfa_num}.

We do not claim that large regular expressions are easy to maintain,
simply that the difficulty in maintaining them is somewhat lesser than
with tables in text format. Also, one advantage of regular expressions
with respect to tables, is that they can be read from left to right on
a line ---~their complexity stems from the flattening of the second
dimension found in the 2D nature of transition diagrams.

If regular expressions are programs, this entails that a lexer and a
parser have to be built to process them, a compilation process whose
output would be a transition table. We can define regular expressions
with regular expressions, and then use a lexer generator based on
regular expressions to generate a lexer for regular expressions, but
our method would become circular and meaningless as a
definition. Fortunately, it is not too difficult to write by hand a
lexer and a parser for simple regular expressions.

We need now a way to build transition diagrams from regular
expressions.

The construction we present here to build a transition diagram from a
regular expression is called \emph{Thompson's construction}. Let us
first associate a transition diagram to the basic (atomic) regular
expressions.
\begin{itemize}

  \item For the expression \(\epsilon\), construct the following
    diagram, where~\(i\) and~\(f\) are new states:
  \begin{center}
    \includegraphics[bb=48 710 135 730]{thompson_epsilon}
  \end{center}

  \item For \(a \in \Sigma\), construct the following diagram,
    where~\(i\) and~\(f\) are new states:
  \begin{center}
    \includegraphics[bb=48 710 135 730]{thompson_symbol}
  \end{center}

\end{itemize}
Now let us associate transition diagrams to complex regular
expressions. In the following, let us assume that~\(\mathcal{D}(s)\)
and~\(\mathcal{D}(t)\) are the diagrams for regular expressions
\(s\)~and~\(t\).
\begin{itemize}

  \item For the regular expression \(st\), construct the following
    diagram \(\mathcal{D}(st)\), where no new state is created:
\begin{center}
\includegraphics[bb=65 660 295 714]{thompson_conc}
\end{center}
  The final state of \(\mathcal{D}(s)\) becomes a normal state, as
  well as the initial state of \(\mathcal{D}(t)\). This way only
  remains a unique initial state~\(i\) and a unique final state~\(f\).

  \item For the regular expression \(s\) \disj \(t\), construct the
    following diagram \(\mathcal{D}(s \, \text{\disj} \, t)\)
\begin{center}
\includegraphics[bb=65 590 272 715,scale=0.9]{thompson_disj}
\end{center}
where \(i\) and \(f\) are new states. Initial and final states of
\(\mathcal{D}(s)\) and \(\mathcal{D}(t)\) become normal.

  \item For the regular expression \(s\)\kleene, construct the
    following diagram \(\mathcal{D}(s\text{\kleene})\), where~\(i\)
    and~\(f\) are new states:
\begin{center}
\includegraphics[bb=50 620 255 718]{thompson_kleene}
\end{center}
Note that we added two \(\epsilon\) transitions and that the initial
and final states of \(\mathcal{D}(s)\) become normal states.

\end{itemize}
How do we apply these simple rules when we have a complex regular
expression, having many level of nested parentheses and other
constructs? Actually, the abstract syntax tree of the regular
expression directs the application of the rules. If the syntax tree
has the shape shown in \fig~\vref{fig:re_ast_conc},
\begin{figure}[b]
\centering
\subfloat[\(s \cdot t\)\label{fig:re_ast_conc}]{
  \includegraphics{re_ast_conc}
}
\qquad
\subfloat[\(s \,\text{\disj}\, t\)\label{fig:re_ast_disj}]{
\includegraphics{re_ast_disj}
}
\qquad
\subfloat[\(s\text{\kleene}\)\label{fig:re_ast_kleene}]{
\includegraphics{re_ast_kleene}
}
\caption{Three tree patterns for three regular expressions}
\end{figure}
then we construct first \(\mathcal{D}(s)\), \(\mathcal{D}(t)\) and
finally \(\mathcal{D}(s \cdot t)\). If the syntax tree has the shape
found in \fig~\vref{fig:re_ast_disj}, then we construct first
\(\mathcal{D}(s)\), \(\mathcal{D}(t)\) and finally \(\mathcal{D} (s
\, \text{\disj} \, t)\). If the syntax tree has the shape shown in
\fig~\vref{fig:re_ast_kleene}, then we construct first \({\mathcal
  D}(s)\) and finally \(\mathcal{D}(s\text{\kleene})\). These pattern
matchings are applied first at the root of the abstract syntax tree of
the regular expression.

\section{Lexical analysis with \ocamllex}

In this section we show how to use \ocamllex, a tool distributed with
the compiler for \OCaml, which takes a specification for a lexer and
outputs \OCaml code implementing that specification. As mentioned
earlier, the LIGO compiler used \ocamllex to generate its lexer.

With \ocamllex, the regular expressions defining the lexemes have a
traditional form, but characters occur between quotes, after the
convention of \OCaml, for example, \textsf{['a'-'z']\texttt{+}
  ['a'-'z' 'A'-'Z' '0'-'9' '\_']*} instead of the usual
\textsf{[a-z]\texttt{+} [a-zA-Z0-9\_]*}. The \OCaml type representing
internally the lexemes is generally not defined in the lexer
specification, which has the file extension \texttt{.mll}. For
instance, that type could be
\begin{tabbing}
 \Xtype \type{token} \= \equal \= \Tint \Xof \type{int} \vbar{}
 \Tident \Xof \type{string} \vbar{} \Ttrue \vbar{} \Tfalse\\
 \> \vbar \> \Tplus \vbar{} \Tminus \vbar{} \Ttimes \vbar{} \Tslash
 \vbar{} \Tequal \vbar{} \Tarrow\\
 \> \vbar \> \Tlpar \vbar{} \Trpar \vbar{} \Tlet \vbar{} \Tin \vbar{} \Trec\\
 \> \vbar \> \Tfun \vbar{}
 \Tif \vbar{} \Tthen \vbar{} \Telse \vbar{} \Tand \vbar{} \Tor \vbar{}
 \Tnot \vbar{} \Teof
\end{tabbing}
Note that it is a good practice to always have a special lexeme \Teof
to denote the end of the file. The specification of a lexer in
\ocamllex follows the general shape
\begin{tabbing}
\{ \emph{Optional \OCaml code as a prologue} \}\\
\Xlet \(r\sb{1}\) \equal \emph{regexp}\\
\ldots\\
\Xlet \(r\sb{p}\) \equal \emph{regexp}\\
\Xrule \= \emph{rule}\(\sb{1}\) \(x\sb{1,1} \dots\, x\sb{1,m}\) \equal \Xparse\\
\> \ \ \ \emph{regexp}\(\sb{1,1}\) \{ \emph{\OCaml code known as
  \emph{action}} \}\\
\> \vbar{} \ \ldots\\
\> \vbar{} \ \emph{regexp}\(\sb{1,n}\) \{ \emph{\OCaml code known as \emph{action}} \}\\
\Xand \= \emph{rule}\(\sb{2}\) \(x\sb{2,1} \dots\, x\sb{2,m}\) \equal \Xparse\\
\> \ldots\\
\Xand \ldots\\
\{ \emph{Optional \OCaml code as an epilogue} \}
\end{tabbing}
Consider the following example:
\begin{tabbing}
\{ \= \Xopen \cst{Parser}\\
   \> \Xexception \cst{Illegal\_char} \Xof \type{string} \}\\
\\
\Xlet \ident{ident} \equal \textsf{['a'-'z'] ['\_' 'A'-'Z' 'a'-'z' '0'-'9']*}\\
\Xrule \= \ident{token} \equal \Xparse\\
  \> \ \ \textsf{['{\tt\char`\ }' '\(\backslash\)n' '\(\backslash\)t'
   '\(\backslash\)r']} \= \{ \ident{token} \ident{lexbuf} \}\\
  \> \vbar{} \str{let} \> \{ \Tlet \}\\
  \> \vbar{} \str{rec} \> \{ \Trec \}\\
  \> \vbar{} \str{=}   \> \{ \Tequal \}\\
  \> \ldots \\
  \> \vbar{} \ident{ident} \Xas \ident{id} \> \{ \Tident \ident{id} \}\\
  \> \vbar{} \textsf{['0'-'9']\texttt{+}} \Xas \ident{n} \> \{ \Tint
     \lpar\ident{int\_of\_string} \ident{n}\rpar{} \}\\
  \> \vbar{} \ident{eof} \> \{ \Teof \}\\
  \> \vbar{} {\large \_} \Xas \ident{c} \> \{ \ident{raise}
     \lpar\cst{Illegal\_char} \ident{c} \rpar{} \}
\end{tabbing}
The prologue opens the module \textsf{Parser} because it contains the
definition of the type \textsf{token}, whose data constructors are
applied in the actions (\cst{LET}, \cst{REC}, etc.). This style is
often used in conjunction with the parsers produced by \menhir or
\ocamlyacc. If we specify a standalone lexer (for example, for
performing unit testing), we then would have a module \textsf{Token}
containing the definition of the lexemes.

Exceptions used in the actions and/or the epilogue are declared in the
prologue --~here we have \textsf{Illegal\_char}.

A regular expression called \textsf{ident} is defined, as well as a
unique parser \textsf{token}. Note that, although the \ocamllex
keyword is \textbf{\textsf{parse}}, it declares a lexer. The rules are
introduced by the keyword \textbf{\textsf{rule}} and, in the actions,
the rules are seen to be functions whose first arguments, like
\(x_{1,1} \dots x_{1,m}\), are arbitrary \OCaml values, then the next
argument is the lexing buffer (matched after the keyword
\textbf{\textsf{parse}}), always implicitly called \textsf{lexbuf}. An
example of this is the action \texttt{\{token lexbuf\}}, which is
typical when we want to skip some characters from the input. This
recursive call works because, in the action, the characters recognised
by the corresponding regular expression have been implicitly removed
from the input stream.

The module \textsf{Lexing} of the standard library of \OCaml contains
some functions whose aim is to manipulate the input stream of
characters. For example, to create an input stream of characters from
the standard input, we would write: \Xlet \ident{char\_flow} \equal{}
\ident{Lexing.from\_channel} \ident{stdin} \Xin{} \ldots

There is a built\hyp{}in regular expression named \ident{eof} which
filters the end of file. It is recommended to match it in order to
produce a \emph{virtual token} \Teof because the implicit behaviours
of the applications with respect to the end of file may vary from one
operating system to another. (See below for another reason.)

Notice too a special regular expression `\textsf{\large \_}' which
matches any kind of character. The order of the regular expressions
matters, therefore this particular expression must be the last one,
otherwise any subsequent expression would be ignored.

If the \ocamllex specification is a file named \textsf{lexer.mll},
then the compilation will take place in two steps:
\begin{enumerate*}

   \item \texttt{ocamllex lexer.mll} will generate either an error or
     \textsf{lexer.ml}; then

   \item \texttt{ocamlc -c lexer.ml} will produce either an error or
     the compiled units \textsf{lexer.cmo} and \textsf{lexer.cmi}, the
     latter only if there is no interface \textsf{lexer.mli} for the
     lexer.

\end{enumerate*}
In theory, the actions linked to the regular expressions are not
compelled to return a lexeme, as the programmer may seek to write a
standalone preprocessor, for example, instead of the combination of a
lexer and a parser, as usually found in compilers. In any case, the
resulting \OCaml code has the shape
\begin{tabbing}
\emph{Prologue}\\
\Xlet \= \Xrec \emph{rule}\(\sb{1}\) \(x\sb{1,1} \dots\, x\sb{1,m}\) \ident{lexbuf} \equal\\
\> \ldots{} \= \Xmatch \ldots{} \Xwith\\
\>\> \; \ldots{} \(\rightarrow\) \emph{action}\\
\>\> \vbar{} \ldots{}\\
\>\> \vbar{} \ldots{} \(\rightarrow\) \emph{action}\\
\Xand \emph{rule}\(\sb{2}\) \(x\sb{2,1} \dots\, x\sb{2,m}\)
\ident{lexbuf} \equal\\
\> \ldots\\
\Xand \ldots\\
\emph{Epilogue}
\end{tabbing}
where \texttt{lexbuf} has the type \textsf{Lexing.lexbuf}.

\paragraph{Lexing inline comments}

Comments are recognised during lexical analysis, but they are usually
discarded. Some lexers examine the contents of comments, looking for
instance for metadata or embedded comments, and thus may signal errors
inside those. The simplest type of comments is that of~\Cpp{}, whose
scope is the rest of the line after it starts:
\begin{tabbing}
\Xrule \= \ident{token} \equal \Xparse\\
\> \ \ \ \ldots\\
\> \vbar{} \textsf{"//"\ \ \ [\symbol{94} '\(\backslash\)n']*
  \ \ '\(\backslash\)n'?} \{ \ident{token} \ident{lexbuf} \}
\end{tabbing}
The regular expression identifies the comment opening, then skips any
character different from an end of line, and finally terminates by
reading an optional end of line. (We assume that the underlying
operating system is \Unix, so an end of line can also terminate a
file.)

\paragraph{Lexing non-nested block comments}

In order to analyse non\hyp{}embedded block comments, we need a more
complex specification:
\begin{tabbing}
\{ \ldots{} \Xexception \cst{Open\_comment} \}\\
\\
\Xrule \= \ident{token} \equal \Xparse\\
\> \ \ \ \ldots\\
\> \vbar{} \str{/*} \{ \ident{in\_comment} \ident{lexbuf} \}\\
\Xand \= \ident{in\_comment} \equal \Xparse\\
\> \ \ \ \str{*/} \= \{ \ident{token} \ident{lexbuf} \}\\
\> \vbar{} \ident{eof} \> \{ \ident{raise} \cst{Open\_comment} \}\\
\> \vbar{} {\large \_} \> \{ \ident{in\_comment} \ident{lexbuf} \}
\end{tabbing}
The rule \textsf{token} recognises the comment opening and its action
calls the additional rule \textsf{in\_comment}, which skips all
characters until the closing of the block and signals an error if the
closing is missing (open comment). When the block is closed, and since
a comment does not yield a lexeme in our context, we need to perform a
recursive call to \textsf{token} to get one --~This is the other
reason why we need the virtual token \Teof, as alluded to previously.

\paragraph{Lexing nested block comments}

Comments in the language~\Clang can be nested, which allows the
programmer to temporarily comment out pieces of source code that may
already contain block comments. If these comments were not nestable,
we could write a single regular expression to recognise them, but,
above, we chose not to do so for readability's sake and to easily
signal the lack of a proper closing. In the nested case, no such
expression can exist, on theoretical grounds: regular languages cannot
be well parenthesised. Informally, this can be understood as: `Regular
expressions cannot count.', in particular, the current level of
nesting cannot be maintained throughout block openings and
closings. To achieve this, we need to rely on the actions, where
function calls are available. The technique consists in modifying the
rule \textsf{in\_comment} in such a manner that the actions become
functions whose argument is the current depth of nesting.
\begin{tabbing}
\Xrule \= \ident{token} \equal \Xparse\\
\> \ \ \ \ldots\\
\> \vbar{} \str{/*} \{ \ident{in\_comment} \ident{lexbuf} \underline{\num{1}} \}\\
\Xand \= \ident{in\_comment} \equal \Xparse\\
\> \ \ \ \str{*/} \= \{ \underline{\Xfun \ident{depth} \(\rightarrow\)} \=
\underline{\Xif \ident{depth} \equal \num{1} \Xthen} \ident{token} \ident{lexbuf}\\
\> \> \> \underline{\Xelse \ident{in\_comment} \ident{lexbuf}
\lpar\ident{depth}\texttt{-}\num{1}\rpar{}} \}\\
\> \underline{\vbar{} \str{/*}} \> \underline{\{ \Xfun \ident{depth}
  \(\rightarrow\) \ident{in\_comment} \ident{lexbuf}
  \lpar\ident{depth}\texttt{+}\num{1}\rpar{} \}}\\
\> \vbar{} \ident{eof} \> \{ \ident{raise} \cst{Open\_comment} \}\\
\> \vbar{} {\LARGE \_} \> \{ \ident{in\_comment} \ident{lexbuf} \}
\end{tabbing}
Note that \Xfun \ident{depth} \(\rightarrow\) \ident{raise}
\cst{Open\_comment} would be less efficient, and the call
\ident{in\_comment} \ident{lexbuf} is equivalent to \Xfun
\ident{depth} \(\rightarrow\) \ident{in\_comment} \ident{lexbuf}
\ident{depth}.

\paragraph{Finite automata}

Lexer generators like \ocamllex work by combining the regular
expressions of the specification, and translating them into a program
expressed in the target language. In order to do so, these have to be
translated first into a formalism of same level of expressivity, but
more intuitive: finite automata. Then, the automaton resulting from
combining several automata is compiled into source code. As we
indicated at the opening of this chapter, we will not discuss automata
theory here, as only basic notions are necessary four our present
purpose, and the Annex presents the topic independently of any
programming language. Therefore, suffice it to give some examples
about the recognition of some lexemes specific to the matter at hand
here.
\begin{itemize}

   \item a keyword:
     \begin{center}
       \includegraphics[bb=48 710 198 730]{mots_cles}
     \end{center}

  \item an integer
    \begin{center}
      \includegraphics[bb=47 709 216 738]{entiers}
    \end{center}

  \item either a keyword or an integer:
    \begin{center}
      \includegraphics[bb=47 687 216 730]{mots_cles_ou_entiers}
    \end{center}

\end{itemize}
If a final state (doubly\hyp{}circled) is reached from the initial
state, the lexeme is recognised, like \Tlet and \Tint.

The lexer considers at all times two pieces of information:
the current state in the specified automaton, and the character at the
head of the input stream.
\begin{itemize*}

  \item If there exists a transition for the head character at the
    current state, then
    \begin{itemize*}

      \item it is withdrawn from the input stream and discarded;

      \item the current state becomes the one pointed to by the
        transition;

      \item the process resumes by considering the new state and the
        new character at the head of the input;

    \end{itemize*}

  \item otherwise, if there is no transition (the state is blocking),
    then
    \begin{itemize*}

      \item if the current state is final, then the associated lexeme
        is emitted;

      \item else, an error is signalled (invalid character).

    \end{itemize*}

\end{itemize*}
Some ambiguity may occur, like
\begin{itemize*}

   \item the input string \str{let} being recognised as a variable
     instead of a keyword,

   \item the input string \str{letrec} being recognised as the
     following lists of lexemes: \lbra\Tlet; \Tident \str{rec}\rbra{}
     or \lbra\Tlet; \Trec\rbra{} or \lbra\Tident \str{letrec}\rbra{}
     etc.

\end{itemize*}
The general solution consists in establishing rules of priority:
\begin{itemize}

   \item when several lexemes are possible prefixes of the input, chose
     the longest;

   \item otherwise, follow the order of definition of the tokens, for
     example, in the \ocamllex specification given earlier, the rule
     for \Tlet is written \emph{before} that for \ident{ident}.

\end{itemize}
This way, the sentence \texttt{let letrec = 3 in 1 + funny} is
recognised as the list \lbra\Tlet; \Tident \str{letrec}; \Tequal;
\Tint \num{3}; \Tin; \Tint \num{1}; \Tplus; \Tident \str{funny}\rbra.

To implement this `longest\hyp{}match rule', we need to add a
structure: an initially empty buffer of characters, and resume the
previous algorithm. When the current state is final and a transition
is enabled, instead of discarding the corresponding character, we save
it in that extra buffer until a blocking state. If that state is
final, we return the associated lexeme, else we emit the lexeme of the
last final state encountered, the characters of the buffer are placed
back into the entrant stream, and we loop back to the initial state.
\begin{center}
\begin{minipage}{0.45\linewidth}
\includegraphics[bb=48 658 198 738]{lexeme_long}
\end{minipage}
\hspace*{15mm}
\begin{minipage}{0.4\linewidth}
$\begin{aligned}
  e_1 &= \texttt{['a'-'k' 'n'-'z']}\\
  e_2 &= \texttt{['a'-'d' 'f'-'z']}\\
  e_3 &= \texttt{['a'-'s' 'u'-'z']}\\
  e_4 &= \texttt{['a'-'z]}
\end{aligned}$
\end{minipage}
\end{center}

% Bibliographical references

\bibliography{guide}
\nocite*{}

\end{document}
